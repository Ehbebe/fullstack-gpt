{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi'), AIMessage(content='How are you?')]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"How are you?\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='1'),\n",
       "  AIMessage(content='1'),\n",
       "  HumanMessage(content='2'),\n",
       "  AIMessage(content='2'),\n",
       "  HumanMessage(content='3'),\n",
       "  AIMessage(content='3'),\n",
       "  HumanMessage(content='4'),\n",
       "  AIMessage(content='4')]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True,\n",
    "    k=4,  # ëŒ€í™” ê¸°ë¡ì„ 4ê°œê¹Œì§€ë§Œ ì €ì¥\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "add_message(1, 1)\n",
    "add_message(2, 2)\n",
    "add_message(3, 3)\n",
    "add_message(4, 4)\n",
    "\n",
    "# í˜„ì¬ ëŒ€í™” ìƒíƒœë¥¼ ë‚˜íƒ€ë‚´ëŠ” inputs ì¸ìë¥¼ í¬í•¨í•˜ì—¬ ë©”ëª¨ë¦¬ ë³€ìˆ˜ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "# ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œë¥¼ ìœ„í•´ ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì‹¤ì œ ì‚¬ìš© ì‹œì—ëŠ” í˜„ì¬ ëŒ€í™” ìƒíƒœì— ëŒ€í•œ ì •ë³´ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "current_inputs = {}\n",
    "memory.load_memory_variables(current_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='2'),\n",
       "  AIMessage(content='2'),\n",
       "  HumanMessage(content='3'),\n",
       "  AIMessage(content='3'),\n",
       "  HumanMessage(content='4'),\n",
       "  AIMessage(content='4'),\n",
       "  HumanMessage(content='5'),\n",
       "  AIMessage(content='5')]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(5, 5)\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ìŠ¤íƒ€íŠ¸ì—… ìŠ¤í…Œì´ì…˜ ê°•ì‚¬ ì´í˜¸ì§„ ì…ë‹ˆë‹¤. ì €ëŠ” ì„œìš¸ì— ì‚´ì•„ìš”.\", \"ì„œìš¸ì— ì‚°ë‹¤ëŠ” ê²ƒì€ ë„ˆë¬´ í–‰ë³µí•©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"ë˜í•œ, ì„œìš¸ì€ ì—¬ëŸ¬ ì¥ì†Œë“¤ì´ ì•„ë¦…ë‹µìŠµë‹ˆë‹¤.\", \"ê·¸ë˜ì„œ ê°€ë³´ê³  ì‹¶ì€ ê³³ì´ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'The human greets the AI in Korean and introduces themselves as a startup station instructor living in Seoul. The AI responds that living in Seoul brings great happiness because there are so many amazing places to visit, making them want to explore even more.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ì´í˜¸ì§„ ì…ë‹ˆë‹¤. ì €ëŠ” ì„œìš¸ì— ì‚´ì•„ìš”.'),\n",
       "  AIMessage(content='ì„œìš¸ì— ì‚°ë‹¤ëŠ” ê²ƒì€ ë„ˆë¬´ í–‰ë³µí•©ë‹ˆë‹¤.')]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=150,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ì´í˜¸ì§„ ì…ë‹ˆë‹¤. ì €ëŠ” ì„œìš¸ì— ì‚´ì•„ìš”.\", \"ì„œìš¸ì— ì‚°ë‹¤ëŠ” ê²ƒì€ ë„ˆë¬´ í–‰ë³µí•©ë‹ˆë‹¤.\")\n",
    "\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"ë˜í•œ, ì„œìš¸ì€ ì—¬ëŸ¬ ì¥ì†Œë“¤ì´ ì•„ë¦…ë‹µìŠµë‹ˆë‹¤.\", \"ê·¸ë˜ì„œ ê°€ë³´ê³  ì‹¶ì€ ê³³ì´ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ì´í˜¸ì§„ ì…ë‹ˆë‹¤. ì €ëŠ” ì„œìš¸ì— ì‚´ì•„ìš”.'),\n",
       "  AIMessage(content='ì„œìš¸ì— ì‚°ë‹¤ëŠ” ê²ƒì€ ë„ˆë¬´ í–‰ë³µí•©ë‹ˆë‹¤.'),\n",
       "  HumanMessage(content='ë˜í•œ, ì„œìš¸ì€ ì—¬ëŸ¬ ì¥ì†Œë“¤ì´ ì•„ë¦…ë‹µìŠµë‹ˆë‹¤.'),\n",
       "  AIMessage(content='ê·¸ë˜ì„œ ê°€ë³´ê³  ì‹¶ì€ ê³³ì´ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤!')]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"ì„œìš¸ì—ì„œ ë³¼ë¼ë””ë³´ìŠ¤í†¡ê¹Œì§€ ì–¼ë§ˆë‚˜ ê±¸ë¦¬ë‚˜ìš”?\", \"ìŒ.. ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='The human greets the AI in Korean and introduces themselves as Lee Hojin living in Seoul.'),\n",
       "  AIMessage(content='ì„œìš¸ì— ì‚°ë‹¤ëŠ” ê²ƒì€ ë„ˆë¬´ í–‰ë³µí•©ë‹ˆë‹¤.'),\n",
       "  HumanMessage(content='ë˜í•œ, ì„œìš¸ì€ ì—¬ëŸ¬ ì¥ì†Œë“¤ì´ ì•„ë¦…ë‹µìŠµë‹ˆë‹¤.'),\n",
       "  AIMessage(content='ê·¸ë˜ì„œ ê°€ë³´ê³  ì‹¶ì€ ê³³ì´ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤!'),\n",
       "  HumanMessage(content='ì„œìš¸ì—ì„œ ë³¼ë¼ë””ë³´ìŠ¤í†¡ê¹Œì§€ ì–¼ë§ˆë‚˜ ê±¸ë¦¬ë‚˜ìš”?'),\n",
       "  AIMessage(content='ìŒ.. ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤.')]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"ì„œìš¸ì—ì„œ ì˜¤ìŠ¤íŠ¸ë¦¬ì•„ ê³ ì‚¬ìš°ê¹Œì§€ ì–¼ë§ˆë‚˜ ê±¸ë¦¬ë‚˜ìš”?\", \"ìŒ.. ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='The human greets the AI in Korean and introduces themselves as Lee Hojin living in Seoul. The AI expresses happiness at living in Seoul, and the human agrees, mentioning that Seoul has many amazing places.'),\n",
       "  AIMessage(content='ê·¸ë˜ì„œ ê°€ë³´ê³  ì‹¶ì€ ê³³ì´ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤!'),\n",
       "  HumanMessage(content='ì„œìš¸ì—ì„œ ë³¼ë¼ë””ë³´ìŠ¤í†¡ê¹Œì§€ ì–¼ë§ˆë‚˜ ê±¸ë¦¬ë‚˜ìš”?'),\n",
       "  AIMessage(content='ìŒ.. ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤.'),\n",
       "  HumanMessage(content='ì„œìš¸ì—ì„œ ì˜¤ìŠ¤íŠ¸ë¦¬ì•„ ê³ ì‚¬ìš°ê¹Œì§€ ì–¼ë§ˆë‚˜ ê±¸ë¦¬ë‚˜ìš”?'),\n",
       "  AIMessage(content='ìŒ.. ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤.')]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conversation Knowledge Graph Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationKGMemory(\n",
    "    llm=llm,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "add_message(\"What is the capital of France?\", \"Paris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': []}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"input\" : \"What is France\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"ì €ëŠ” ìŠ¤íƒ€íŠ¸ì—…ìº í¼ìŠ¤ ê°œë°œê³¼ì • ê°•ì‚¬ ì´í˜¸ì§„ì…ë‹ˆë‹¤.\", \"ì œê°€ ê°€ì§„ ì§€ì‹ì„ ê³µìœ í•˜ëŠ” ê²ƒì€ ë„ˆë¬´ ì¬ë°‹ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': []}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"input\" : \"ìŠ¤íƒ€íŠ¸ì—…ìº í¼ìŠ¤ ê°œë°œê³¼ì • ê°•ì‚¬ ì´í˜¸ì§„ ì¢‹ì•„í•˜ëŠ” ê²ƒì€?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On ì´í˜¸ì§„: ì´í˜¸ì§„ is a ê°œë°œê³¼ì • ê°•ì‚¬. ì´í˜¸ì§„ is a ìŠ¤íƒ€íŠ¸ì—…ìº í¼ìŠ¤. ì´í˜¸ì§„ is a ê°•ì‚¬. ì´í˜¸ì§„ ì·¨ë¯¸ëŠ” ì½”ë”©. ì´í˜¸ì§„ ì·¨ë¯¸ëŠ” ìŒì•… ê°ìƒ.')]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì´í˜¸ì§„ì— ëŒ€í•œ ì •ë³´ ì¶”ê°€\n",
    "add_message(\"ì €ëŠ” ìŠ¤íƒ€íŠ¸ì—…ìº í¼ìŠ¤ ê°œë°œê³¼ì • ê°•ì‚¬ ì´í˜¸ì§„ì…ë‹ˆë‹¤.\", \"ì´í˜¸ì§„ì€ ê°œë°œì„ ê°€ë¥´ì¹˜ëŠ” ê²ƒì„ ì¢‹ì•„í•©ë‹ˆë‹¤.\")\n",
    "add_message(\"ì´í˜¸ì§„ì˜ ì·¨ë¯¸ëŠ” ì½”ë”©ê³¼ ìŒì•… ê°ìƒì…ë‹ˆë‹¤.\", \"ì´í˜¸ì§„ì€ ì—¬ê°€ ì‹œê°„ì— ì½”ë”©ì„ í•˜ê±°ë‚˜ ìŒì•…ì„ ë“£ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ì •ë³´ ìš”ì²­\n",
    "memory.load_memory_variables({\"input\" : \"ìŠ¤íƒ€íŠ¸ì—…ìº í¼ìŠ¤ ê°œë°œê³¼ì • ê°•ì‚¬ ì´í˜¸ì§„ ì¢‹ì•„í•˜ëŠ” ê²ƒì€?\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Memory on LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3më‚´ ì´ë¦„ì€ ì´í˜¸ì§„ ì…ë‹ˆë‹¤.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ì•ˆë…•í•˜ì„¸ìš”, ì´í˜¸ì§„ë‹˜. ë§Œë‚˜ì„œ ë°˜ê°€ì›Œìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=100,\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=PromptTemplate.from_template(\"{question}\"),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.predict(question=\"ë‚´ ì´ë¦„ì€ ì´í˜¸ì§„ ì…ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3më‚˜ëŠ” ì„œìš¸ì—ì„œ ì‚´ê³  ìˆìŠµë‹ˆë‹¤.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ì„œìš¸ì€ ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ì´ì ê°€ì¥ í° ë„ì‹œë¡œì„œ ë¬¸í™”, ì—­ì‚¬, ì‡¼í•‘, ìŒì‹ ë“± ë‹¤ì–‘í•œ ì¦ê¸¸ ê±°ë¦¬ê°€ ë§ì€ ë„ì‹œì…ë‹ˆë‹¤. ì„œìš¸ì—ì„œëŠ” í•œêµ­ ì „í†µ ë¬¸í™”ë¥¼ ì²´í—˜í•  ìˆ˜ ìˆëŠ” ê¶ê¶ì´ë‚˜ ì‚¬ì°°, í˜„ëŒ€ì ì¸ ê±´ì¶•ë¬¼ê³¼ ì‡¼í•‘ëª°, ë‹¤ì–‘í•œ ìŒì‹ì„ ë§›ë³¼ ìˆ˜ ìˆëŠ” ë ˆìŠ¤í† ë‘ ë“± ë‹¤ì–‘í•œ ì¦ê¸¸ ê±°ë¦¬ê°€ ìˆìŠµë‹ˆë‹¤. ë˜í•œ ëŒ€ì¤‘êµí†µì´ ì˜ ë°œë‹¬í•´ ìˆì–´ ì´ë™ì´ í¸ë¦¬í•˜ê³  ì•ˆì „í•œ ë„ì‹œì´ê¸°ë„ í•©ë‹ˆë‹¤. ì„œìš¸ì—ì„œëŠ” ë‹¤ì–‘í•œ ë¬¸í™” í–‰ì‚¬ë‚˜ ì¶•ì œë„ ë§ì´ ì—´ë¦¬ë¯€ë¡œ ì¦ê±°ìš´ ì‹œê°„ì„ ë³´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì„œìš¸ì—ì„œì˜ ìƒí™œì€ í•­ìƒ ìƒˆë¡œìš´ ê²½í—˜ê³¼ ë°œê²¬ì´ ê¸°ë‹¤ë¦¬ê³  ìˆì–´ í¥ë¯¸ë¡œìš´ ë„ì‹œì…ë‹ˆë‹¤.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"ë‚˜ëŠ” ì„œìš¸ì—ì„œ ì‚´ê³  ìˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3më‚´ ì´ë¦„ì´ ë¬´ì—‡ì´ë‹ˆ?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ì£„ì†¡í•©ë‹ˆë‹¤, ì €ëŠ” ë‹¹ì‹ ì˜ ì´ë¦„ì„ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì´ë¦„ì„ ì•Œë ¤ì£¼ì‹œê² ì–´ìš”?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"ë‚´ ì´ë¦„ì´ ë¬´ì—‡ì´ë‹ˆ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3më‚´ ì´ë¦„ì€ ì´í˜¸ì§„ ì…ë‹ˆë‹¤.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ì•ˆë…•í•˜ì„¸ìš”, ì´í˜¸ì§„ë‹˜. ë§Œë‚˜ì„œ ë°˜ê°€ì›Œìš”! ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=100,\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=PromptTemplate.from_template(\"{question}\"),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.predict(question=\"ë‚´ ì´ë¦„ì€ ì´í˜¸ì§„ ì…ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3më‚´ ì´ë¦„ì´ ë¬´ì—‡ì´ë‹ˆ?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ì£„ì†¡í•©ë‹ˆë‹¤, ì €ëŠ” ë‹¹ì‹ ì˜ ì´ë¦„ì„ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì´ë¦„ì„ ì•Œë ¤ì£¼ì‹œê² ì–´ìš”?'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"ë‚´ ì´ë¦„ì´ ë¬´ì—‡ì´ë‹ˆ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: The human introduces themselves as Lee Hojin. The AI responds in Korean, greeting Lee Hojin and asking how it can help.\\nHuman: ë‚´ ì´ë¦„ì´ ë¬´ì—‡ì´ë‹ˆ?\\nAI: ì£„ì†¡í•©ë‹ˆë‹¤, ì €ëŠ” ë‹¹ì‹ ì˜ ì´ë¦„ì„ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì´ë¦„ì„ ì•Œë ¤ì£¼ì‹œê² ì–´ìš”?'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    \n",
      "    Human:ë‚´ ì´ë¦„ì€ ì´í˜¸ì§„ ì…ë‹ˆë‹¤.\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ì•ˆë…•í•˜ì„¸ìš”, ì´í˜¸ì§„ë‹˜. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "template = \"\"\"\n",
    "    You are a helpful AI talking to a human.\n",
    "\n",
    "    {chat_history}\n",
    "    Human:{question}\n",
    "    You:\n",
    "\"\"\"\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=PromptTemplate.from_template(template),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.predict(question=\"ë‚´ ì´ë¦„ì€ ì´í˜¸ì§„ ì…ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    Human: ë‚´ ì´ë¦„ì€ ì´í˜¸ì§„ ì…ë‹ˆë‹¤.\n",
      "AI: ì•ˆë…•í•˜ì„¸ìš”, ì´í˜¸ì§„ë‹˜. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?\n",
      "    Human:ë‚˜ëŠ” ì„œìš¸ì—ì„œ ì‚´ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ê·¸ë ‡êµ°ìš”, ì„œìš¸ì€ ì •ë§ ë©‹ì§„ ë„ì‹œì…ë‹ˆë‹¤. ë¬´ì—‡ì´ í•„ìš”í•˜ì‹ ê°€ìš”? ë¶€ë™ì‚° ì •ë³´ë‚˜ ê´€ê´‘ì§€ ì¶”ì²œ ë“± ë„ì™€ë“œë¦´ ìˆ˜ ìˆëŠ” ê²ƒì´ ìˆë‚˜ìš”?'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"ë‚˜ëŠ” ì„œìš¸ì—ì„œ ì‚´ê³  ìˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    System: The human introduces themselves as ì´í˜¸ì§„.\n",
      "AI: ì•ˆë…•í•˜ì„¸ìš”, ì´í˜¸ì§„ë‹˜. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?\n",
      "Human: ë‚˜ëŠ” ì„œìš¸ì—ì„œ ì‚´ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "AI: ê·¸ë ‡êµ°ìš”, ì„œìš¸ì€ ì •ë§ ë©‹ì§„ ë„ì‹œì…ë‹ˆë‹¤. ë¬´ì—‡ì´ í•„ìš”í•˜ì‹ ê°€ìš”? ë¶€ë™ì‚° ì •ë³´ë‚˜ ê´€ê´‘ì§€ ì¶”ì²œ ë“± ë„ì™€ë“œë¦´ ìˆ˜ ìˆëŠ” ê²ƒì´ ìˆë‚˜ìš”?\n",
      "    Human:ë‚´ ì´ë¦„ì´ ë¬´ì—‡ì´ë‹ˆ?\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'AI: ì£„ì†¡í•©ë‹ˆë‹¤, ì´í˜¸ì§„ë‹˜. ì œê°€ ì´ë¦„ì„ ì•Œ ìˆ˜ ìˆëŠ” ë°©ë²•ì€ ì—†ìŠµë‹ˆë‹¤. ì–´ë–¤ ë„ì›€ì´ í•„ìš”í•˜ì‹ ê°€ìš”?'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"ë‚´ ì´ë¦„ì´ ë¬´ì—‡ì´ë‹ˆ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': 'System: The human introduces themselves as ì´í˜¸ì§„. The AI greets ì´í˜¸ì§„ and asks how it can help. ì´í˜¸ì§„ mentions living in Seoul. The AI acknowledges Seoul as a great city and offers assistance with real estate information or tourist recommendations.\\nHuman: ë‚´ ì´ë¦„ì´ ë¬´ì—‡ì´ë‹ˆ?\\nAI: AI: ì£„ì†¡í•©ë‹ˆë‹¤, ì´í˜¸ì§„ë‹˜. ì œê°€ ì´ë¦„ì„ ì•Œ ìˆ˜ ìˆëŠ” ë°©ë²•ì€ ì—†ìŠµë‹ˆë‹¤. ì–´ë–¤ ë„ì›€ì´ í•„ìš”í•˜ì‹ ê°€ìš”?'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: ë‚´ ì´ë¦„ì€ ì´í˜¸ì§„ ì…ë‹ˆë‹¤.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ì•ˆë…•í•˜ì„¸ìš”, ì´í˜¸ì§„ë‹˜. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "chain.predict(question=\"ë‚´ ì´ë¦„ì€ ì´í˜¸ì§„ ì…ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. LCEL Based Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm\n",
    "\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\": question})\n",
    "    memory.save_context(\n",
    "        {\"input\": question},\n",
    "        {\"output\": result.content},\n",
    "    )\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Hello Hojin Lee, it's nice to meet you too! How can I assist you today?\"\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"Hello, My name is Hojin Lee, Nice to meet you.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Yes, you introduced yourself as Hojin Lee. How can I assist you, Hojin?'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"Do you know my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "class MovieEmojiMemoryManager:\n",
    "    def __init__(self, memory):\n",
    "        self.memory = memory\n",
    "\n",
    "    def load_memory(self, _=None):\n",
    "        return self.memory.load_memory_variables({}).get(\"history\", [])\n",
    "\n",
    "    def save_memory(self, question, emoji):\n",
    "        self.memory.save_context({\"input\": question}, {\"output\": emoji})\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=120, return_messages=True)\n",
    "memory_manager = MovieEmojiMemoryManager(memory)\n",
    "\n",
    "emoji_response = {\n",
    "    \"íƒ‘ê±´\": \"ğŸ›©ï¸ğŸ‘¨â€âœˆï¸ğŸ”¥\",\n",
    "    \"ë§¤íŠ¸ë¦­ìŠ¤\": \"ğŸ’ŠğŸ’»ğŸ•¶ï¸\",\n",
    "    \"í„°ë¯¸ë„¤ì´í„°\": \"ğŸ¤–ğŸ•°ï¸ğŸ”«\",\n",
    "    \"í•´ë¦¬í¬í„°\": \"ğŸ§™â€â™‚ï¸ğŸª„âš¡\",\n",
    "    \"ì•„ë©”ë¦¬ì¹¸ ì…°í”„\": \"ğŸ‘¨â€ğŸ³ğŸ²ğŸ”ª\",\n",
    "    \"ë²”ì£„ë„ì‹œ\": \"ğŸ”«ğŸš“ğŸ’°\",\n",
    "}\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ì˜í™” ì œëª©ì„ ì•Œë ¤ì£¼ì‹œë©´, ê·¸ ì˜í™”ë¥¼ ëŒ€í‘œí•˜ëŠ” ì„¸ ê°œì˜ ì´ëª¨í‹°ì½˜ìœ¼ë¡œ ì‘ë‹µí•´ ë“œë¦½ë‹ˆë‹¤.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "chain = RunnablePassthrough.assign(history=memory_manager.load_memory) | prompt | llm\n",
    "\n",
    "def invoke_chain(movie_title):\n",
    "    question = f\"{movie_title}ì— ëŒ€í•œ ì„¸ ê°œì˜ ì´ëª¨ì§€ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "    selected_emoji = emoji_response.get(movie_title, \"â“â“â“\")\n",
    "    result = chain.invoke({\"question\": question})\n",
    "    memory_manager.save_memory(question, selected_emoji)\n",
    "    print(f\"ì§ˆë¬¸: {question}\\nì‘ë‹µ: {selected_emoji}\")\n",
    "    \n",
    "    print(\"í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœ:\", memory_manager.load_memory())\n",
    "\n",
    "\n",
    "def get_asked_movies():\n",
    "    history_data = memory_manager.load_memory()\n",
    "    if not history_data:\n",
    "        print(\"ì§ˆë¬¸í•œ ì˜í™”ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    movies_info = []\n",
    "    for i in range(0, len(history_data), 2):\n",
    "        # ì§ˆë¬¸ ë‚´ìš©ì´ ì €ì¥ëœ ê°ì²´ì˜ ì†ì„±ì„ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "        question_item = history_data[i]\n",
    "        if hasattr(question_item, 'content'):\n",
    "            question_content = question_item.content\n",
    "            # \"ì— ëŒ€í•œ ì„¸ ê°œì˜ ì´ëª¨ì§€ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\" í¬í•¨ ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "            if \"ì— ëŒ€í•œ ì„¸ ê°œì˜ ì´ëª¨ì§€ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\" in question_content:\n",
    "                movie_title = question_content.split(\"ì— ëŒ€í•œ ì„¸ ê°œì˜ ì´ëª¨ì§€ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\")[0].strip()\n",
    "                # ë‹¤ìŒ ê°ì²´ì—ì„œ ì‘ë‹µ(ì´ëª¨ì§€)ì„ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "                if i+1 < len(history_data) and hasattr(history_data[i + 1], 'content'):\n",
    "                    emoji_response = history_data[i + 1].content\n",
    "                else:\n",
    "                    emoji_response = \"â“â“â“\"\n",
    "                movies_info.append(f\"ì˜í™”: {movie_title}, ì´ëª¨ì§€: {emoji_response}\")\n",
    "\n",
    "    if movies_info:\n",
    "        print(\"\\n\".join(movies_info))\n",
    "    else:\n",
    "        print(\"ì§ˆë¬¸í•œ ì˜í™” ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì§ˆë¬¸: ì•„ë©”ë¦¬ì¹¸ ì…°í”„ì— ëŒ€í•œ ì„¸ ê°œì˜ ì´ëª¨ì§€ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\n",
      "ì‘ë‹µ: ğŸ‘¨â€ğŸ³ğŸ²ğŸ”ª\n",
      "í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœ: [HumanMessage(content='ë§¤íŠ¸ë¦­ìŠ¤ì— ëŒ€í•œ ì„¸ ê°œì˜ ì´ëª¨ì§€ëŠ” ë¬´ì—‡ì¸ê°€ìš”?'), AIMessage(content='ğŸ’ŠğŸ’»ğŸ•¶ï¸'), HumanMessage(content='ì•„ë©”ë¦¬ì¹¸ ì…°í”„ì— ëŒ€í•œ ì„¸ ê°œì˜ ì´ëª¨ì§€ëŠ” ë¬´ì—‡ì¸ê°€ìš”?'), AIMessage(content='ğŸ‘¨\\u200dğŸ³ğŸ²ğŸ”ª')]\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"ì•„ë©”ë¦¬ì¹¸ ì…°í”„\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì§ˆë¬¸: í•´ë¦¬í¬í„°ì— ëŒ€í•œ ì„¸ ê°œì˜ ì´ëª¨ì§€ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\n",
      "ì‘ë‹µ: ğŸ§™â€â™‚ï¸ğŸª„âš¡\n",
      "í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœ: [SystemMessage(content='The human asks what three emojis represent \"The Matrix.\"'), AIMessage(content='ğŸ’ŠğŸ’»ğŸ•¶ï¸'), HumanMessage(content='ì•„ë©”ë¦¬ì¹¸ ì…°í”„ì— ëŒ€í•œ ì„¸ ê°œì˜ ì´ëª¨ì§€ëŠ” ë¬´ì—‡ì¸ê°€ìš”?'), AIMessage(content='ğŸ‘¨\\u200dğŸ³ğŸ²ğŸ”ª'), HumanMessage(content='í•´ë¦¬í¬í„°ì— ëŒ€í•œ ì„¸ ê°œì˜ ì´ëª¨ì§€ëŠ” ë¬´ì—‡ì¸ê°€ìš”?'), AIMessage(content='ğŸ§™\\u200dâ™‚ï¸ğŸª„âš¡')]\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"í•´ë¦¬í¬í„°\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜í™”: ì•„ë©”ë¦¬ì¹¸ ì…°í”„, ì´ëª¨ì§€: ğŸ‘¨â€ğŸ³ğŸ²ğŸ”ª\n",
      "ì˜í™”: í•´ë¦¬í¬í„°, ì´ëª¨ì§€: ğŸ§™â€â™‚ï¸ğŸª„âš¡\n"
     ]
    }
   ],
   "source": [
    "get_asked_movies()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-yIjbGwu1-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
